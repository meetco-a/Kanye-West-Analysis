{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom Functions import *\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define column names\ndfColumns = ['Full Relative Path', 'Year', 'File Name', 'Song Name']\ndfNumCols = len(dfColumns)\n\n# Initialize empty list that we'll use later\ndfNumRows = 0\nfullPaths = []\nsongYears = []\nfileNames = []\nsongNames = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a list of folders in the \"Lyrics\" directory\nyearFolders = [year for year in os.listdir('Lyrics')]\n\n# Loop through each folder and collect the following data\nfor folder in yearFolders:\n    # Number of files in each folder and path of each file\n    dfNumRows = len([f for f in os.listdir(os.path.join('Lyrics', folder))])\n    fullPaths += [(os.path.join('Lyrics', folder, f)) for f in\n                  os.listdir(os.path.join('Lyrics', folder))]\n\n    # The year, file name, and song name for each file\n    songYears += [folder]*dfNumRows\n    fileNames += [f for f in os.listdir(os.path.join('Lyrics', folder))]\n    songNames = [song.replace('.txt', '') for song in fileNames]\n\n# Put the data from above into a dictionary and convert it to a DataFrame\ndfDict = {dfColumns[0]: fullPaths, dfColumns[1]: songYears, dfColumns[2]: fileNames, dfColumns[3]: songNames}\ndfCorpus = pd.DataFrame(dfDict)\n\n# Get file lengths for each song/file and add it to corpus index\nfileLengths = get_corpus_file_lengths(dfCorpus)\ndfCorpus['File Lengths'] = fileLengths","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# I've defined two lexicons: I-words (words referring to oneself); and grandeur words\n# Here I load each lexicon and then count the occurrence of its words in each song\nlexiconI = pd.read_table(os.path.join('Lexicons', 'iwords.txt'), index_col=0, sep='\\t')\nlexiconGrand = pd.read_table(os.path.join('Lexicons', 'grandeurwords.txt'), index_col=0, sep='\\t')\n\n# Extract the list of regex patterns in each lexicon\npatternListI = make_pattern_list(lexiconI['Regex'])\npatternListGrand = make_pattern_list(lexiconGrand['Regex'])\n\n# Count the number of pattern matches in each file for both lexicons and put them in a DataFrame\nfileSeries = dfCorpus['Full Relative Path']\nmatchCountI = match_patterns_with_files(patternListI, fileSeries)\nmatchCountGrand = match_patterns_with_files(patternListGrand, fileSeries)\ncountIDF = df_pattern_matches(dfCorpus, matchCountI, lexiconI)\ncountGrandDF = df_pattern_matches(dfCorpus, matchCountGrand, lexiconGrand)\n\n# Finally, count total lexicon words per song and add new columns to the corpus\ntotalIWords = countIDF.sum(axis=1)\ntotalGrandWords = countGrandDF.sum(axis=1)\ndfCorpus[\"I-words\"] = totalIWords\ndfCorpus[\"Grandeur words\"] = totalGrandWords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# The next measure we will use is the vocabulary size, i.e. number of unique words per song\nvocabSize = pd.Series(np.zeros(len(fileSeries)))\n\n# Get the lyrics of each song\nfor i in range(len(fileSeries)):\n    lyrics = ''\n    with open(fileSeries[i], 'rb') as f:\n        for line in f:\n            lineB = line.decode(errors='replace')\n            lineC = lineB.strip('\\n')\n            lyrics += ' ' + lineC\n\n    # Make a bag of words from the lyrics and count number of unique words\n    aBOW = make_conventional_bow(lyrics)\n    vocabSize[i] = len(aBOW)\n\ndfCorpus[\"Vocabulary size\"] = vocabSize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# The final measure we will use is lexical density, i.e. the ratio of non-stopwords to total words in a song\nstops = stopwords.words('english')\n\nlexicalDensity = pd.Series(np.zeros(len(fileSeries)))\nfor i in range(len(fileSeries)):\n    aText = ''\n    with open(fileSeries[i], 'rb') as f:\n        for line in f:\n            lineb = line.decode(errors='replace')\n            linec = lineb.strip('\\n')\n            aText += linec\n    aBOW = make_conventional_bow(aText)\n    totalWords = sum(aBOW.values())\n    nonStopWords = 0\n    dictKeys = list(aBOW.keys())\n    for entry in dictKeys:\n        if entry not in stops:\n            nonStopWords += aBOW[entry]\n    lexicalDensity[i] = (nonStopWords/totalWords)*100\n\ndfCorpus['Lexical density'] = lexicalDensity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finally, we get average I-words/Grandeur words/Vocabulary size per year\ndfCorpusYear = dfCorpus[[\"Year\", \"I-words\", \"Grandeur words\", \"Vocabulary size\", \"Lexical density\"]]\\\n    .groupby([\"Year\"]).mean()\ndfCorpusYear.reset_index(inplace=True)\n\n# Since there are a few years with no data (i.e. no songs released), we need to impute the missing data\n# First I add each missing year to the DF with NaN values\nfor year in range(2004, 2021):\n    if str(year) not in dfCorpusYear['Year'].values:\n        dfTemp = pd.DataFrame([[str(year), np.NaN, np.NaN, np.NaN, np.NaN]], columns=list(dfCorpusYear.columns))\n        dfCorpusYear = dfCorpusYear.append(dfTemp, ignore_index=True)\n\n# Then we sort and re-index the data, and finally fill in the missing values using linear interpolation\ndfCorpusYear.sort_values(by=['Year'], inplace=True)\ndfCorpusYear.reset_index(drop=True, inplace=True)\ndfCorpusYear.interpolate(method='linear', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we plot the data\n# First we plot I-words and Grandeur words\nfig, ax1 = plt.subplots()\n\ncolor = 'tab:red'\nax1.set_xlabel('Year')\nax1.set_ylabel('I-words', color=color)\nax1.plot(dfCorpusYear[\"Year\"], dfCorpusYear[\"I-words\"], color=color)\nax1.tick_params(axis='y', labelcolor=color)\n\n# Instantiate a second axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\nax2.set_ylabel('Grandeur words', color=color)\nax2.plot(dfCorpusYear[\"Year\"], dfCorpusYear[\"Grandeur words\"], color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Then plot vocab. size and lexical density\nfig, ax1 = plt.subplots()\n\ncolor = 'tab:red'\nax1.set_xlabel('Year')\nax1.set_ylabel('Vocabulary size', color=color)\nax1.plot(dfCorpusYear[\"Year\"], dfCorpusYear[\"Vocabulary size\"], color=color)\nax1.tick_params(axis='y', labelcolor=color)\n\n# Instantiate a second axes that shares the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:blue'\nax2.set_ylabel('Lexical density', color=color)\nax2.plot(dfCorpusYear[\"Year\"], dfCorpusYear[\"Lexical density\"], color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nfig.tight_layout()\nplt.yscale(\"log\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}